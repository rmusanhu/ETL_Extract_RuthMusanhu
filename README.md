**Student Name**: Ruth Musanhu
**Student ID**: 670474 
**Submission Date**: [10th june 2025]


## ğŸ“Œ Project Overview
The project is a practical demonsration of Extract-Transform-Load (ETL) operations focusing on **full** and **incremental** data extraction patterns.It was developed using Python within a Jupyter Notebook environment, it processes synthetic sales transaction data to simulate a real-world data pipeline scenario.


## ğŸ› ï¸ Tools & Technologies
- **Python 3**
- **Pandas** for Data manipulation
- **Jupyter Notebook** for Interactive development
- **Git/GitHub** for Version control

## ğŸ“‚ Project Structure
ETL_Extract_ruthmusanhu
â”œâ”€â”€ etl_extract.ipynb         # Your Jupyter notebook containing ETL logic
â”œâ”€â”€ custom_data.csv           # The chosen/generated sales dataset
â”œâ”€â”€ last_extraction.txt       # Stores the timestamp for incremental extraction
â”œâ”€â”€ .gitignore                # Excludes unneeded files from version control
â”œâ”€â”€ README.md                 # This project explanation file
â”œâ”€â”€ image.png               # Screenshot: Full Extraction Output
â”œâ”€â”€ image-1.png               # Screenshot: Incremental Extraction Output
â””â”€â”€ README.pdf                # PDF version of the README (if applicable)


## Dataset Origin
The dataset `custom_data.csv` is synthetically generated and contains 100 sales records with timestamps.




## ğŸ§© Key Features
1. **Full Extraction**:
Full Extraction is the process of extracting all data from the source system every time the ETL process runs. It does not consider whether the data has changed or not.In this case the  notebook displays key statistics like the total number of rows and columns, along with a sample of the loaded data,confirming a successful full extraction.

![alt text](image.png)


2. **Incremental Extraction**
Incremental Extraction retrieves only the new or changed data since the last extraction. It uses a mechanism such as timestamps, log files, or change data capture (CDC) to identify what has changed. This method is more efficient for large datasets as it minimizes data movement and processing


![alt text](image-1.png)

3. **Save timestamp**
Updates the timestamp after successful extraction.


## How to Reproduce
1. Open `etl_extract.ipynb` in Jupyter Notebook.
2. Run all cells in order.
3. Ensure `custom_data.csv` and `last_extraction.txt` are in the same directory.



## LAB 4

## ğŸ“˜ Overview  
This project adds on the previous project with two more CSVs being added (transformed_full.csv and transformed_incremental.csv). Its objectives were to build and enhance an ETL (Extract, Transform, Load) pipeline that can extract data from a CSV file using both full and incremental extraction methods, apply multiple data transformation techniques to clean, enrich, and categorize the data, and output structured datasets ready for analysis.




## ğŸ”„ ETL Flow

### 1. **Full Extraction**
This involved reading the entire dataset. From there some transformations were applied to the dataset. The transformations done include cleaning of missing values, enrichment by adding a new column (discounted price) and then cartegorizing the data. The results were then saved to the transformed_full.csv.
![alt text](tranformed_data.png)
![alt text](transformed_full_csv.png)


### 2. **Incremental Extraction**
This filters the records for records newer than 'last_extraction.txt'. The same transformations as in full extraction are also applied. The 'last_extraction.txt' is updated after each run. The results were saved in the tranformed_incremental.csv file


![alt text](incremental_data.png)
![alt text](incremental_csv.png)



## ğŸ”§ Transformations Applied

âœ… **Cleaning:**  
- Dropped rows with missing values.

âœ… **Enrichment:**  
- Added a `discounted_price` column (10% discount on `total_price`).

âœ… **Categorization:**  
- Grouped `quantity` into 3 bins: Low (0â€“2), Medium (2â€“4), High (>4).


## ğŸ“ How to Run

1. Ensure `custom_data.csv` and `last_extraction.txt` exist.
2. Open and run `etl_extract.ipynb` in Jupyter Notebook.
3. Check generated outputs:
   - `transformed_full.csv`
   - `transformed_incremental.csv`

---

## ğŸ§¾ Notes
- Add new rows with a later `transaction_date` in `custom_data.csv` to test incremental logic.
- Timestamp format in `last_extraction.txt`: `YYYY-MM-DD HH:MM:SS`








